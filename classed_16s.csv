Abstract,Topic,DOI,GPT_response,GPT_class,Input_tokens,Output_tokens,reason_stop,system_fingerprint
"Artificial Intelligence (AI) has grown dramatically over the past few decades and has much greater influence today on human performance in every walk of life. AI is great for accelerating our work, but there are also downsides to it. One such downside is the application of Facial Recognition Technologies (FRT) available today, which has adverse consequences like racial discrimination or wrongful judgement by commercial firms and even law enforcement organizations. The landmark 'Gender Shades' project [1] in 2018 showed a highly skewed error bar between facial recognition accuracies of subjects who are black females compared to white males using an intersectional comparison approach between facial recognition software by IBM, Microsoft and Face++. This supplemented previous studies which highlighted issues with gender classification or on the whole misidentification issues posed by facial recognition technologies (FRT) [2]. This algorithm bias towards misgendering or misclassification based on skin color could be attributed to a highly unbalanced training datasets that lack diversity as well as a lack of understanding of the black box machine learning (ML) models that are used to build the FRTs. From a Code of Ethics perspective, such algorithms do not conform to the fundamental canons [3] such as prioritizing the safety, and welfare of the public. Furthermore, they cannot be used honorably, responsibly and ethically to enhance the reputation, and usefulness of the companies promoting these technologies because they are unable to provide truthful and objective information regarding facial recognition. A possible remedy could be to curate an exhaustively diverse dataset w.r.t both color and gender. However, the task of building such a dataset would require access to a diverse population, which is not always possible in a multi-racial and multi-ethnic society. We thus propose an exhaustive review-based study of existing work on the introduction of explainable AI [4],[5] (XAI) techniques in such ML models to understand how the model itself learns. This study would explore what constraints to put on the learning method of the model, which can enforce reduction in misclassification errors from gender and skin color thus enforcing various ethical aspects like taking care of algorithm bias, introducing transparency and accountability. It would also explore and underline the overall social impact improved FRTs might have from a code of ethics perspective.  © 2024 IEEE.",16,10.1109/SIEDS61124.2024.10534745,"This paper discusses the adverse effects of facial recognition technologies (FRT) on racial and gender discrimination, highlighting biases in algorithmic predictions due to unbalanced training datasets. It proposes a review of existing work on explainable AI (XAI) techniques to understand and mitigate these biases, emphasizing ethical considerations and social impacts. The focus on analyzing the implications of AI on fairness and ethical outcomes within an economic context aligns it with Class A. Class: A",A,1811,89,stop,fp_0ba0d124f1
"Artificial intelligence (AI) has had a significant impact on several sectors and fields. Failure Modes and Effects Analysis (FMEA) is a powerful risk management and prevention tool that can help companies identify and address any weaknesses in their practices. The Risk Priority Number (RPN) approach has been criticized for its flaws since it calculates the product of severity, occurrence, and detection ratings of threats. By considering a variety of criteria and aspects, and by thoughtfully and carefully combining expert perspectives, the proposed FMEA model may more accurately evaluate the risks associated with AI in Education. Picture Fuzzy Sets (PFSs) and Grey Relational Analysis-TOPSIS (GRA-TOPSIS) are combined to achieve this. The goal of this research is to use an updated FMEA model to assess and rank the risk hierarchy of the seven identified hazards. According to the data, algorithmic risk is the most significant issue that requires urgent attention. Relevant constraints and suggestions are also provided to encourage research on the dangers of AI in Education. Suggestions include creating a national or international regulatory authority to control the use of artificial intelligence in education, promoting the use of the FMEA technique as a common framework for assessing the risks connected to AI in Education. © 2024 Informa UK Limited, trading as Taylor & Francis Group.",16,10.1080/00036846.2024.2321835,"This paper focuses on improving the Failure Modes and Effects Analysis (FMEA) model to assess risks associated with AI in education. It critiques the traditional Risk Priority Number (RPN) approach and proposes a new model using Picture Fuzzy Sets and Grey Relational Analysis-TOPSIS. The emphasis is on risk assessment and management rather than analyzing the ethical implications of AI in an economic context. Therefore, it should be classified as Class B. Class: B",B,1610,89,stop,fp_0ba0d124f1
"The article addresses the issue that is widely discussed in Germany and other jurisdictions: can the management of a company use AI applications in its decision-making process without violating its fiduciary duties? The lack of transparency in conventional AI applications conflicts with the fiduciary duty to check the plausibility of external expert advice (in Germany known as the ISION principles). This tension can be partly resolved by using explainable AI (XAI). In this work, we review the basic principles of machine learning and XAI and discuss them in the legal context. © 2023 Walter de Gruyter GmbH. All rights reserved.",16,10.1515/ecfr-2023-0033,"This paper explores the intersection of AI applications and fiduciary duties in corporate decision-making, particularly focusing on the transparency issues associated with conventional AI. It discusses how explainable AI (XAI) can help resolve these tensions within a legal framework. The analysis of AI's implications for ethical and legal responsibilities in management aligns it with economic decision-making contexts. Class: A",A,1474,72,stop,fp_0ba0d124f1
"As AI has a wide range of influence on human social life, issues of transparency and ethics of AI are emerging. In particular, it is widely known that due to the existence of historical bias in data against ethics or regulatory frameworks for fairness, trained AI models based on such biased data could also impose bias or unfairness against a certain sensitive group (e.g., non-white, women). Demographic disparities due to AI, which refer to socially unacceptable bias that an AI model favors certain groups (e.g., white, men) over other groups (e.g., black, women), have been observed frequently in many applications of AI and many studies have been done recently to develop AI algorithms which remove or alleviate such demographic disparities in trained AI models. In this paper, we consider a problem of using the information in the sensitive variable for fair prediction when using the sensitive variable as a part of input variables is prohibitive by laws or regulations to avoid unfairness. As a way of reflecting the information in the sensitive variable to prediction, we consider a two-stage procedure. First, the sensitive variable is fully included in the learning phase to have a prediction model depending on the sensitive variable, and then an imputed sensitive variable is used in the prediction phase. The aim of this paper is to evaluate this procedure by analyzing several benchmark datasets. We illustrate that using an imputed sensitive variable is helpful to improve prediction accuracies without hampering the degree of fairness much. © 2022. The Korean Statistical Society, and Korean International Statistical Society. All rights reserved.",16,10.29220/CSAM.2022.29.2.251,This paper addresses the issue of demographic disparities in AI predictions due to historical bias in data. It proposes a two-stage procedure to incorporate sensitive variables in a way that complies with legal and ethical standards while still improving prediction accuracy. The focus on evaluating the fairness and ethical implications of AI aligns it with economic outcomes. Class: A,A,1660,64,stop,fp_0ba0d124f1
"Robo-advisors (RAs) support economic decisions for customers using artificial intelligence (AI). RAs are gaining increasing significance but lack market penetration. A significant issue is the perceived low transparency of such AI systems. This study examines the public’s demands on RAs with text-mining methods from the perspective of explainable artificial intelligence (XAI). The results reveal understandability and trustworthiness issues for each of the RA use phases (configuration, matching, and maintenance). In particular, five barriers emerge in RA if information needs remain unanswered: entry barrier, assessment barrier, evaluation barrier, continuation barrier and withdrawal barrier. The barriers can be mitigated by combining explanation, design and communication measures. The results are discussed regarding theoretical implications and practical recommendations for facilitating the adoption of RAs. JEL: D8 (D81, D83, D89), G1 (G11), G2 (G20, G23), G4 (G41), I2 (I24, I25), O3 (O31, O33) © 2022 The Author(s).",16,10.1177/02601079221130183,"This paper focuses on the transparency issues surrounding robo-advisors (RAs) and how these affect public trust and adoption. It employs text-mining methods to identify barriers to understanding and trust in RAs, proposing measures to enhance explainability and communication. While it discusses the implications of AI in economic decision-making, it primarily addresses methods to improve transparency rather than analyzing the impacts of AI on fairness or ethical outcomes. Thus, it should be classified as Class B. Class: B",B,1570,96,stop,fp_0ba0d124f1
"Though companies are building artificial intelligence (AI) systems and integrating them into business operations, executives are concerned about AI’s distinctive challenges (e.g., opacity) and seeking to develop new capabilities in response. We describe a new AIX explanation capability that companies must establish before their AI initiatives can thrive. This capability has four dimensions: decision tracing, bias remediation, boundary setting and value formulation. Together, these dimensions help organizations to address the challenges of model opacity, model drift, acting mindlessly and the unproven nature of AI © 2022. University of Minnesota",16,10.17705/2msqe.00063,"This paper discusses the challenges companies face when integrating AI into their operations, particularly focusing on the need for a new explanation capability (AIX) to address issues like opacity and bias. It outlines four dimensions of this capability that organizations should develop to enhance their AI initiatives. Since the paper proposes a method for mitigating challenges rather than analyzing the impacts of AI on fairness or ethical outcomes, it should be classified as Class B. Class: B",B,1464,86,stop,fp_0ba0d124f1
"The black-box nature of Artificial Intelligence (AI) models and their associated explainability limitations create a major adoption barrier. Explainable Artificial Intelligence (XAI) aims to make AI models more transparent to address this challenge. Researchers and practitioners apply XAI services to explore relationships in data, improve AI methods, justify AI decisions, and control AI technologies with the goals to improve knowledge about AI and address user needs. The market volume of XAI services has grown significantly. As a result, trustworthiness, reliability, transferability, fairness, and accessibility are required capabilities of XAI for a range of relevant stakeholders, including managers, regulators, users of XAI models, developers, and consumers. We contribute to theory and practice by deducing XAI archetypes and developing a user-centric decision support framework to identify the XAI services most suitable for the requirements of relevant stakeholders. Our decision tree is founded on a literature-based morphological box and a classification of real-world XAI services. Finally, we discussed archetypical business models of XAI services and exemplary use cases. © 2022, The Author(s).",16,10.1007/s12525-022-00603-6,"This paper focuses on the challenges of AI adoption due to the black-box nature of AI models and the need for explainability. It proposes a user-centric decision support framework for selecting suitable explainable AI (XAI) services, emphasizing the importance of trustworthiness and fairness for various stakeholders. While it addresses the need for fairness in AI, it primarily focuses on developing a framework rather than analyzing the impacts of AI on fairness or ethical outcomes. Thus, it should be classified as Class B. Class: B",B,1572,100,stop,fp_0ba0d124f1
"We make two contributions to understanding the role of algorithms in regulatory enforcement. First, we illustrate how big-data analytics can inadvertently import private biases into public policy. We show that a much-hyped use of predictive analytics – using consumer data to target food-safety enforcement – can disproportionately harm Asian establishments. Second, we study a solution by Pope and Sydnor (2011), which aims to debias predictors via marginalization, while still using information of contested predictors. We find the solution may be limited when protected groups have distinct predictor distributions, due to model extrapolation. Common machine-learning techniques heighten these problems. © 2018 Mohr Siebeck.",16,10.1628/jite-2019-0001,"This paper examines the unintended consequences of using big-data analytics in regulatory enforcement, specifically how predictive analytics can perpetuate biases against certain demographic groups, such as Asian establishments in food safety enforcement. It also evaluates a proposed solution to mitigate these biases through debiasing techniques. The focus on the implications of algorithmic bias and its effects on fairness within a regulatory context aligns it with Class A. Class: A",A,1482,80,stop,fp_0ba0d124f1
"Artificial intelligence (AI) is pervading the government and transforming how public services are provided to consumers across policy areas spanning allocation of government benefits, law enforcement, risk monitoring, and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI failures? In 13 preregistered studies (N = 3,724) across a range of policy areas, the authors show that algorithmic failures are generalized more broadly than human failures. This effect is termed “algorithmic transference” as it is an inferential process that generalizes (i.e., transfers) information about one member of a group to another member of that same group. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm to a greater extent than failure information about a person is transferred to another person. Capturing AI's impact on consumers and societies, these results show how the premature or mismanaged deployment of faulty AI technologies may undermine the very institutions that AI systems are meant to modernize. © American Marketing Association 2022.",16,10.1177/00222437221110139,"This paper examines consumer responses to failures in AI systems across various public service areas, introducing the concept of ""algorithmic transference,"" where failures of one AI system are generalized to others. It utilizes a series of preregistered studies to analyze how perceptions of AI versus human failures differ. The focus on the implications of AI failures on consumer trust and institutional integrity aligns it with economic outcomes. Class: A",A,1614,81,stop,fp_0ba0d124f1
"Why do biased algorithmic predictions arise, and what interventions can prevent them? We examine this topic with a field experiment about using machine learning to predict human capital. We randomly assign approximately 400 AI engineers to develop software under different experimental conditions to predict standardized test scores of OECD residents. We then assess the resulting predictive algorithms using the realized test performances, and through randomized audit-like manipulations of algorithmic inputs. We also used the diversity of our subject population to measure whether demographically non-traditional engineers were more likely to notice and reduce algorithmic bias, and whether algorithmic prediction errors are correlated within programmer demographic groups. This document describes our experimental design and motivation; the full results of our experiment are available at https://ssrn.com/abstract=3615404. © 2020 Owner/Author.",16,10.1145/3391403.3399545,This paper investigates the origins of biased algorithmic predictions and explores interventions to mitigate them through a field experiment involving AI engineers. It assesses the performance of predictive algorithms and examines the influence of demographic diversity among engineers on bias detection and reduction. The focus on understanding and analyzing algorithmic bias within a social context aligns it with economic implications. Class: A,A,1512,68,stop,fp_0ba0d124f1
"Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals. © The Author(s) 2017.",16,10.1093/qje/qjx032,"This paper evaluates the potential of machine learning to enhance human decision-making in bail decisions, focusing on the complexities of comparing algorithmic predictions to judicial decisions. It employs econometric strategies to assess welfare gains from integrating machine learning into the judicial process, highlighting reductions in crime and racial disparities. The analysis emphasizes the economic implications of using AI in decision-making contexts. Class: A",A,1660,73,stop,fp_0ba0d124f1
"Problem definition: Data analytics models and machine learning algorithms are increasingly deployed to support consequential decision-making processes, from deciding which applicants will receive job offers and loans to university enrollments and medical interventions. However, recent studies show these models may unintentionally amplify human bias and yield significant unfavorable decisions to specific groups. Methodology/ results: We propose a distributionally robust classification model with a fairness constraint that encourages the classifier to be fair in the equality of opportunity criterion. We use a type-∞ Wasserstein ambiguity set centered at the empirical distribution to represent distributional uncertainty and derive a conservative reformulation for the worst-case equal opportunity unfairness measure. We show that the model is equivalent to a mixed binary conic optimization problem, which standard off-the-shelf solvers can solve. We propose a convex, hinge-loss-based model for large problem instances whose reformulation does not incur binary variables to improve scalability. Moreover, we also consider the distributionally robust learning problem with a generic ground transportation cost to hedge against the label and sensitive attribute uncertainties. We numerically examine the performance of our proposed models on five real-world data sets related to individual analysis. Compared with the state-of-the-art methods, our proposed approaches significantly improve fairness with negligible loss of predictive accuracy in the testing data set. Managerial implications: Our paper raises awareness that bias may arise when predictive models are used in service and operations. It generally comes from human bias, for example, imbalanced data collection or low sample sizes, and is further amplified by algorithms. Incorporating fairness constraints and the distributionally robust optimization (DRO) scheme is a powerful way to alleviate algorithmic biases. © 2024 INFORMS.",16,10.1287/msom.2022.0230,"This paper addresses the issue of algorithmic bias in decision-making processes supported by data analytics and machine learning. It proposes a distributionally robust classification model with fairness constraints to mitigate bias, demonstrating its effectiveness through numerical experiments on real-world datasets. The focus on analyzing the impacts of AI on fairness within an economic context aligns it with Class A. Class: A",A,1686,70,stop,fp_0ba0d124f1
"The rapid proliferation of Artificial Intelligence (AI) applications in various domains of our lives has prompted a need for a shift towards a human-centered and trustworthy approach to AI. In this study we employ the Assessment List for Trustworthy Artificial Intelligence (ALTAI) checklist to evaluate the trustworthiness of Artificial Intelligence for Student Performance Prediction (AI4SPP), an AI-powered system designed to detect students at risk of school failure. We strongly support the ethical and legal development of AI and propose an implementation design where the user can choose to have access to each level of a three-tier outcome bundle: the AI prediction alone, the prediction along with its confidence level, and, lastly, local explanations for each grade prediction together with the previous two information. AI4SPP aims to raise awareness among educators and students regarding the factors contributing to low school performance, thereby facilitating the implementation of interventions not only to help students, but also to address biases within the school community. However, we also emphasize the ethical and legal concerns that could arise from a misuse of the AI4SPP tool. First of all, the collection and analysis of data, which is essential for the development of AI models, may lead to breaches of privacy, thus causing particularly adverse consequences in the case of vulnerable individuals. Furthermore, the system's predictions may be influenced by unacceptable discrimination based on gender, ethnicity, or socio-economic background, leading to unfair actions. The ALTAI checklist serves as a valuable self-assessment tool during the design phase of AI systems, by means of which commonly overlooked weaknesses can be highlighted and addressed. In addition, the same checklist plays a crucial role throughout the AI system life cycle. Continuous monitoring of sensitive features within the dataset, alongside survey assessments to gauge users’ responses to the systems, is essential for gathering insights and intervening accordingly. We argue that adopting a critical approach to AI development is essential for societal progress, believing that it can evolve and accelerate over time without impeding openness to new technologies. By aligning with ethical principles and legal requirements, AI systems can make significant contributions to education while mitigating potential risks and ensuring a fair and inclusive learning environment. © 2024 The Authors",16,10.1016/j.clsr.2024.105986,"This paper evaluates the trustworthiness of an AI system designed for predicting student performance, emphasizing ethical and legal considerations in AI development. It utilizes the ALTAI checklist to assess potential biases and privacy concerns, advocating for a human-centered approach to AI in education. The focus on ethical implications and fairness in AI applications aligns it with economic outcomes in educational contexts. Class: A",A,1784,73,stop,fp_0ba0d124f1
"Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.  © 2024 ACM.",16,10.1145/3630106.3659029,"This paper addresses the issue of model-induced distribution shifts (MIDS) in machine learning, which can perpetuate biases and unfairness in predictive models. It introduces a framework to track MIDS and proposes algorithmic reparation (AR) as a method to mitigate these negative impacts. The focus on analyzing the implications of AI on fairness and accountability within an economic context aligns it with Class A. Class: A",A,1587,81,stop,fp_0ba0d124f1
"The rising adoption of learning analytics and academic performance prediction technologies in higher education highlights the urgent need for transparency and explainability. This demand, rooted in ethical concerns and fairness considerations, converges with Explainable Artificial Intelligence (XAI) principles. Despite the recognized importance of transparency and fairness in learning analytics, empirical studies examining student fairness perceptions, particularly within academic performance prediction, remain limited. We conducted a pre-registered factorial survey experiment involving 1,047 German students to investigate how decision tree features (simplicity and accuracy) influence perceived distributive and informational fairness, mediated by causability (i.e., the self-assessed understandability of a machine learning model's cause-effect linkages). Additionally, we examined the moderating role of institutional trust in these relationships. Our results indicate that decision tree simplicity positively affects fairness perceptions, mediated by causability. In contrast, prediction accuracy neither directly nor indirectly influences these perceptions. Even if the hypothesized effects of interest are either minor or non-existent, results show that the medium positive effect of causability on the distributive fairness assessment depends on institutional trust. These findings substantially impact the crafting of transparent machine learning models in educational settings. We discuss important implications for fairness and transparency in implementing academic performance prediction systems.  © 2024 Owner/Author.",16,10.1145/3630106.3658953,"This paper investigates the perceptions of fairness in academic performance prediction technologies among students, focusing on the influence of decision tree features like simplicity and accuracy. It employs a factorial survey experiment to analyze how these features affect perceived fairness, mediated by the understandability of the model. The study emphasizes the importance of transparency and fairness in educational AI applications, aligning it with ethical outcomes in an economic context. Class: A",A,1608,79,stop,fp_0ba0d124f1
"Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ""generality""comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and 'general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.  © 2024 Owner/Author.",16,10.1145/3630106.3658542,"This paper systematically compares the environmental costs of generative, multi-purpose AI systems versus task-specific machine learning models, focusing on energy consumption and carbon emissions during inference. It highlights the significant environmental impact of deploying general-purpose models and calls for a more careful consideration of their utility against these costs. The analysis does not focus on fairness or ethical outcomes in an economic context, thus it should be classified as Class B. Class: B",B,1612,84,stop,fp_0ba0d124f1
"The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear. We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.  © 2024 Owner/Author.",16,10.1145/3630106.3658966,"This paper evaluates the performance and feasibility of smaller, open-weight generative AI models compared to high-parameter, closed-weight models in various settings, particularly focusing on issues of bias, privacy, and adaptability. It conducts experiments to assess the trade-offs between reproducibility and performance, emphasizing the implications for transparency and ethical considerations in AI deployment. The analysis of these factors within an economic context aligns it with Class A. Class: A",A,1650,85,stop,fp_0ba0d124f1
"An increasing number of regulations propose 'AI audits' as a mechanism for achieving transparency and accountability for artificial intelligence (AI) systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently lacks agreed-upon practices, procedures, taxonomies, and standards. We propose the 'criterion audit' as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations' ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which 'bias audits' can be performed for in-scope hiring algorithms, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering a critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion - informed by experiences in performing these audits in practice - highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of audits.  © 2024 ACM.",16,10.1145/3630106.3658957,"This paper proposes a framework for conducting AI audits, specifically the 'criterion audit,' to ensure transparency and accountability in AI systems. It draws parallels with financial auditing practices and aims to establish standards for compliance and assurance in AI, particularly regarding bias in hiring algorithms. The focus is on developing a method for auditing rather than analyzing the impacts of AI on fairness or ethical outcomes. Therefore, it should be classified as Class B. Class: B",B,1623,86,stop,fp_0ba0d124f1
"'Scale the model, scale the data, scale the GPU farms' is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.  © 2024 Owner/Author.",16,10.1145/3630106.3658968,This paper evaluates the impact of dataset scaling on racial and gender bias in visio-linguistic models (VLMs) trained on large datasets. It presents empirical findings on how scaling affects misclassification rates of images based on race and gender. The focus on the implications of dataset scaling for fairness and bias in AI aligns it with economic outcomes related to algorithmic decision-making. Class: A,A,1774,78,stop,fp_0ba0d124f1
"Recent regulatory efforts, including Executive Order 14110 and the AI Bill of Rights, have focused on mitigating discrimination in AI systems through novel and traditional application of anti-discrimination laws. While these initiatives rightly emphasize fairness testing and mitigation, we argue that they pay insufficient attention to robust bias measurement and mitigation - and that without doing so, the frameworks cannot effectively achieve the goal of reducing discrimination in deployed AI models. This oversight is particularly concerning given the instability and brittleness of current algorithmic bias mitigation and fairness optimization methods, as highlighted by growing evidence in the algorithmic fairness literature. This instability heightens the risk of what we term discrimination-hacking or d-hacking, a scenario where, inadvertently or deliberately, the selection of models based on favorable fairness metrics within specific samples could lead to misleading or non-generalizable fairness performance. We term this effect d-hacking because systematically selecting among numerous models to find the least discriminatory one parallels the concept of p-hacking in social science research of selectively reporting outcomes that appear statistically significant resulting in misleading conclusions. In light of these challenges, we argue that AI fairness regulation should not only call for fairness measurement and bias mitigation, but also specify methods to ensure robust solutions to discrimination in AI systems. Towards the goal of arguing for robust fairness assessment and bias mitigation in AI regulation, this paper (1) synthesizes evidence of d-hacking in the computer science literature and provides experimental demonstrations of d-hacking, (2) analyzes current legal frameworks to understand the treatment of robust fairness and non-discriminatory behavior, both in recent AI regulation proposals and traditional U.S. discrimination law, and (3) outlines policy recommendations for preventing d-hacking in high-stakes domains.  © 2024 Owner/Author.",16,10.1145/3630106.3658928,"This paper critiques current regulatory efforts aimed at mitigating discrimination in AI systems, emphasizing the need for robust bias measurement and mitigation strategies. It introduces the concept of ""d-hacking,"" where model selection based on favorable fairness metrics can lead to misleading outcomes. The methodology includes synthesizing literature on d-hacking, analyzing legal frameworks, and providing policy recommendations. The focus on the implications of AI fairness and discrimination within a regulatory context aligns it with Class A. Class: A",A,1699,92,stop,fp_0ba0d124f1
"Deploying an algorithmically informed policy is a significant intervention in society. Prominent methods for algorithmic fairness focus on the distribution of predictions at the time of training, rather than the distribution of social goods that arises after deploying the algorithm in a specific social context. However, requiring a 'fair' distribution of predictions may undermine efforts at establishing a fair distribution of social goods. First, we argue that addressing this problem requires a notion of prospective fairness that anticipates the change in the distribution of social goods after deployment. Second, we provide formal conditions under which this change is identified from pre-deployment data. That requires accounting for different kinds of performative effects. Here, we focus on the way predictions change policy decisions and, consequently, the causally downstream distribution of social goods. Throughout, we are guided by an application from public administration: the use of algorithms to predict who among the recently unemployed will remain unemployed in the long term and to target them with labor market programs. Third, using administrative data from the Swiss public employment service, we simulate how such algorithmically informed policies would affect gender inequalities in long-term unemployment. When risk predictions are required to be 'fair' according to statistical parity and equality of opportunity, targeting decisions are less effective, undermining efforts to both lower overall levels of long-term unemployment and to close the gender gap in long-term unemployment.  © 2024 Owner/Author.",16,10.1145/3630106.3659020,"This paper critiques existing methods of algorithmic fairness that focus solely on prediction distributions, arguing for a prospective fairness approach that considers the impact of algorithmic decisions on the distribution of social goods post-deployment. It uses administrative data to simulate the effects of algorithmically informed policies on gender inequalities in long-term unemployment. The analysis highlights the potential negative consequences of enforcing fairness in predictions on actual social outcomes. Given its focus on the implications of AI for fairness and social equity within an economic context, it aligns with Class A. Class: A",A,1631,105,stop,fp_0ba0d124f1
"Data sharing in the European Union (EU) has gained new momentum, among others for machine learning (ML) and artificial intelligence (AI) training purposes. By enabling models' training whilst preserving the privacy of data, Privacy Enhancing Technologies (PETs) have therefore gained popularity, especially among policymakers. So far, computer science research has focused on advancing state-of-the-art privacy engineering and exploring trade-offs between privacy and accuracy. Meanwhile, legal scholarship began investigating the challenges arising therefrom. Yet, few works have delved into the fairness implications of PETs. Further research is essential to both prevent the propagation of bias and discrimination and to limit the accumulation of market power within very few economic entities suitable to undermine fair competition and consumer rights. In our work, we will address this knowledge gap by adopting a legal and computer science point of view. After scoping our understanding of possible unfair sides of PETs based on technical and socio-legal understandings of fairness (Section 2), we provide an overview of PETs mostly relevant for ML and AI training (Section 3). We then discuss fairness-related challenges arising from their use (Section 4) and we suggest possible technical and regulatory (e.g., impact assessment, new rights) solutions to address the shortcomings identified (Section 5). We finally provide conclusions and ideas for future research (Section 6).  © 2024 Owner/Author.",16,10.1145/3630106.3659024,This paper explores the fairness implications of Privacy Enhancing Technologies (PETs) in the context of data sharing for machine learning and AI in the EU. It addresses the challenges of bias and discrimination while discussing potential technical and regulatory solutions. The focus on fairness and its implications within an economic context aligns it with Class A. Class: A,A,1632,66,stop,fp_0ba0d124f1
"With the rise of artificial intelligence (AI), concerns about AI applications causing unforeseen harms to safety, privacy, security, and fairness are intensifying. While attempts to create regulations are underway, with initiatives such as the EU AI Act and the 2023 White House executive order, skepticism abounds as to the efficacy of such regulations. This paper explores an interdisciplinary approach to designing policy for the explainability of AI applications, as the widely discussed ""right to explanation""associated with the EU General Data Protection Regulation is ambiguous. To develop practical guidance for explainability, we conducted an experimental study that involved continuous collaboration among a team of researchers with AI and policy backgrounds over the course of ten weeks. The objective was to determine whether, through interdisciplinary effort, we can reach consensus on a policy for explainability in AI-one that is clearer, and more actionable and enforceable than current guidelines. We share nine observations, derived from an iterative policy design process, which included drafting the policy, attempting to comply with it (or circumvent it), and collectively evaluating its effectiveness on a weekly basis. Key observations include: iterative and continuous feedback was useful to improve policy drafts over time, discussing evidence of compliance was necessary during policy design, and human-subject studies were found to be an important form of evidence. We conclude with a note of optimism, arguing that meaningful policies can be achieved within a moderate time frame and with limited experience in policy design, as demonstrated by our student researchers on the team. This holds promising implications for policymakers, signaling that practical and effective regulation for AI applications is attainable.  © 2024 Owner/Author.",16,10.1145/3630106.3659028,"This paper addresses the challenges of regulating AI applications, particularly focusing on the need for clear policies regarding AI explainability. It employs an interdisciplinary approach, involving collaboration among researchers to develop actionable guidelines. The study emphasizes the iterative process of policy design and evaluation, aiming to create effective regulations for AI. Since the paper discusses the implications of AI regulation and its impact on fairness and ethical outcomes, it aligns with Class A. Class: A",A,1673,85,stop,fp_0ba0d124f1
"Biases in artificial intelligence (AI), a pressing issue in human-AI interaction, can be exacerbated by AI systems’ opaqueness. This paper reports on our development of a user-centered explainable-AI approach to reducing such opaqueness, guided by the theoretical framework of anthropomorphism and the results of two 3 × 3 between-subjects experiments (n = 207 and n = 223). Specifically, those experiments investigated how, in a gender-biased hiring situation, three levels of AI human-likeness (low, medium, high) and three levels of richness of AI explanation (none, lean, rich) influenced users’ 1) perceptions of AI bias and 2) adoption of AI's recommendations, as well as how such perceptions and adoption varied across participant characteristics such as gender and pre-existing trust in AI. We found that comprehensive explanations helped users to recognize AI bias and mitigate its influence, and that this effect was particularly pronounced among females in a scenario where females were being discriminated against. Follow-up interviews corroborated our quantitative findings. These results can usefully inform explainable AI interface design. © 2024 Elsevier Ltd",16,10.1016/j.ijinfomgt.2024.102775,"This paper focuses on reducing biases in AI through a user-centered explainable AI approach, examining how different levels of AI human-likeness and explanation richness affect user perceptions of bias and adoption of AI recommendations. The methodology includes two experiments assessing the impact of these factors in a gender-biased hiring context. Since it analyzes the implications of AI on fairness and decision-making outcomes, it aligns with Class A. Class: A",A,1588,83,stop,fp_0ba0d124f1
"The fairness of vocational contest scoring is key to generating reliable competency assessments. This study examined the performance impact of the motivation of English-as-a-foreign-language learners in contests with vocabulary knowledge antecedents in the contexts of artificial intelligence (AI) and blockchain (BC). The sample comprised 185 participants of an oral English contest at higher vocational institution in China. AI-powered scoring of learners’ contest performance and a survey were used to collect data. The findings revealed that learners’ intrinsic drive was the main positive factor, outweighing their extrinsic motivation, and that AI and BC increased the trustworthiness and integrity of contest records, thus providing new opportunities to build learner trust and form psychological incentives. This study enriches foreign language motivation theory in the context of contest research and highlights the importance of using AI and BC to enhance the scoring accuracy and credibility of contests as authoritative evaluation instruments in vocational education. © 2024 IGI Global. All rights reserved.",16,10.4018/JOEUC.336277,"This study focuses on the impact of motivation on the performance of English learners in AI-powered scoring contests. It examines how AI and blockchain enhance the trustworthiness and integrity of scoring, thereby influencing learner motivation and trust. While it discusses the role of AI in improving scoring accuracy, the primary focus is on educational outcomes rather than the ethical implications of AI in an economic context. Thus, it should be classified as Class B. Class: B",B,1539,86,stop,fp_0ba0d124f1
"Recent research shows that automated decision-making systems based on Artificial Intelligence (AI) may lead to perceived unfairness and bias, especially in some sensitive areas. There is no universal debiasing solution for AI applications and systems. This paper proposes a bias-reducing framework based on contextual knowledge graphs for decision-making systems to help analyse and detect potential bias factors during system operation in near-real time. In particular, the contextual knowledge graph is designed to learn the relations between current tasks and corresponding features and explore the correlation among data, context and tasks. Three bias assessment metrics (i.e., label bias, sampling bias and timeliness bias) are proposed to measure, quantify and qualitatively define the bias level for the pre- and post-modelling. The trained model using debiased datasets combines contextual knowledge to support fairer decision-making. Experimental results show that the proposed method is more effective in supporting fairer decision- making than the existing methods. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",16,10.1080/12460125.2024.2349436,"This paper addresses the issue of bias in AI-driven decision-making systems by proposing a framework that utilizes contextual knowledge graphs to analyze and detect bias in near-real time. It introduces metrics for assessing various types of bias and demonstrates that the proposed method enhances fairness in decision-making compared to existing approaches. Since the focus is on developing a method to mitigate bias rather than analyzing its impacts, it should be classified as Class B. Class: B",B,1561,85,stop,fp_0ba0d124f1
"This study examines the accuracy and ethical implications of using convolutional neural networks (CNN) for automated crime detection. A CNN model was trained on a dataset of criminal mugshots to identify potential criminal behaviour based on facial features. This study analyzed the performance of the model and achieved a high accuracy rate in identifying criminals. However, the ethical implications of automated criminal detection are also explored, including bias, privacy and human rights violations. The findings of this study highlight the need for caution and ethical considerations when implementing automated crime detection technologies. It is important to ensure that such technologies are not used to violate the rights of individuals or perpetuate societal biases. © 2023 IEEE.",16,10.1109/RMKMATE59243.2023.10369356,"This study focuses on the use of convolutional neural networks (CNN) for automated crime detection, analyzing both the accuracy of the model and its ethical implications, such as bias and privacy concerns. The paper emphasizes the need for ethical considerations in the implementation of such technologies to prevent violations of rights and societal biases. Since it critically examines the ethical outcomes of AI in an economic context (automated crime detection), it aligns with Class A. Class: A",A,1484,89,stop,fp_0ba0d124f1
"Decision-making is crucial to the performance and well-being of any organization. While artificial intelligence algorithms are increasingly used in the industry for decision-making purposes, the adoption of decision-making techniques to develop new artificial intelligence models does not follow the same trend. Complex artificial intelligence algorithm structures such as gradient boosting, ensembles, and neural networks offer higher accuracy at the expense of transparency. In organizations, however, managers and other stakeholders need to understand how an algorithm came to a given decision to properly criticize, learn from, audit, and improve said algorithms. Among the most recent techniques to address this, explainable artificial intelligence (XAI) algorithms offer a previously unforeseen level of interpretability, explainability, and informativeness to different human roles in the industry. XAI algorithms seek to balance the trade-off between interpretability and accuracy by introducing techniques that, for instance, explain the feature relevance in complex algorithms, generate counterfactual examples in “what-if?” analyses, and train surrogate models that are intrinsically explainable. However, while the trade-off between these two objectives is commonly referred to in the literature, only some proposals use multi-objective optimization in XAI applications. Therefore, this document proposes a new multi-objective optimization application to help decision-makers (for instance, data scientists) to generate new surrogate machine learning models based on black-box models. These surrogates are generated by a multi-objective problem that maximizes, at the same time, interpretability and accuracy. The proposed application also has a multi-criteria decision-making step to rank the best surrogates considering these two objectives. Results from five classification and regression datasets tested on four black-box models show that the proposed method can create simple surrogates maintaining high levels of accuracy. © 2023",16,10.1016/j.ejdp.2023.100040,"This paper focuses on the need for explainable artificial intelligence (XAI) in decision-making processes within organizations. It proposes a multi-objective optimization approach to balance interpretability and accuracy in AI models, allowing decision-makers to generate surrogate models that are both understandable and effective. The emphasis on how AI impacts decision-making and the need for transparency aligns it with economic outcomes. Class: A",A,1702,76,stop,fp_0ba0d124f1
"Fairness is a crucial concept in the context of artificial intelligence ethics and policy. It is an incremental component in existing ethical principle frameworks, especially for algorithm-enabled decision systems. Yet, translating fairness principles into context specific practices can be undermined by multiple unintended organisational risks. This paper argues that there is a gap between the potential and actual realized value of AI. Therefore, this research attempts to answer how organisations can mitigate AI risks that relate to unfair decision outcomes. We take a holistic view by analyzing the challenges throughout a typical AI product life cycle while focusing on the critical question of how rather broadly defined fairness principles may be translated into day-to-day practical solutions at the organizational level. We report on an exploratory case study of a social impact microfinance organization that is using AI-enabled credit scoring to support the screening process to particularly financially marginalized entrepreneurs. This paper highlights the importance of considering the strategic role of the organisation when developing and evaluating fair algorithm- enabled decision systems. The proposed framework and results of this study can be used to inspire the right questions that suit the context an organisation is situated in when implementing fair AI. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",16,10.1007/978-3-031-31671-5_4,"This paper addresses the challenges of implementing fairness principles in AI decision-making within organizations, particularly in the context of a microfinance organization using AI for credit scoring. It explores how organizations can mitigate risks associated with unfair outcomes and translates fairness principles into practical solutions. The focus on mitigating risks rather than analyzing the impacts of AI on fairness suggests it aligns with Class B. Class: B",B,1589,74,stop,fp_0ba0d124f1
"AI is believed to be the most disruptive force in technology in the coming decade. The best intentions, however, can yield negative consequences, that is, the serious problems of introducing AI, especially algorithmic decision-making and its overtrust, into business and society-the resulting discriminatory and biased decisions. Quite a few studies on algorithmic decision-making have made strong claims about the causality between algorithms and biased decisions. Multiple protective measures, including the Explainable Artificial Intelligence, have also been enacted against the discriminatory and biased algorithmic decision-making practices. Nevertheless, they are persistent because of algorithmic obscurity, biased training data, the false belief that algorithms are neutral, and the public's perception that explainable and data-driven decisions are often not objective. This paper proposes a black-box approach to auditing algorithms. The approach draws on the counterfactual theories of causation. It aims at identifying obvious and obscure decision factors engendering decisions from multiple counterfactuals for a given factual. © Issues in Information Systems.",16,10.48009/2_iis_2022_107,"This paper discusses the negative consequences of algorithmic decision-making, particularly focusing on discrimination and bias. It critiques existing protective measures and proposes a black-box auditing approach based on counterfactual theories to identify decision factors contributing to biased outcomes. The emphasis on understanding and analyzing the impacts of AI on fairness and ethical outcomes aligns it with Class A. Class: A",A,1553,70,stop,fp_0ba0d124f1
"Artificial intelligence (AI) is permeating one human endeavor after another. However, there is increasing concern regarding the use of AI: potential biases it contains, as well as mis-judged AI use. This study continues the recent investigations into the biases and issues that are potentially introduced into human decision-making with AI. We experimentally set-up a decision-making classification task and observe human classifiers when they are guided in their decision-making either by AI or other humans. We find that over-reliance or authoritative stigmatization is present when AI is concerned and that with human guidance discursive explanatory decision-making is present. We conclude that while AI is seen as authoritative even in a low stake decision-making setting, it does not suppress choice, but combined with a lack of transparency, AI suppresses visibility into rationality creation by the decision maker. Based on the emergent explorative relationships between types of rationality, AI transparency and authoritativeness, we provide future research avenues based on our findings.  © 2022 IEEE.",16,10.1109/IEEM55944.2022.9989994,This paper examines the biases and issues introduced into human decision-making when using AI compared to human guidance. It employs an experimental setup to analyze the effects of AI's perceived authority and transparency on decision-making processes. The focus on how AI influences decision-making and rationality aligns it with economic implications. Class: A,A,1556,61,stop,fp_0ba0d124f1
"Over the last few years, companies’ investment in new AI systems has seen a strong and constant progression. However, except for the Big Tech, the use of AI is still marginal at this stage, and seems to spark cautiousness and apprehension. A potential reason for this hesitation may be linked to a lack of trust related in particular to the so-called black box AI technologies such as deep learning. This is why our research objective is to explore the effects of explainability on trust in these new AI-based digital systems with which the users can either interact or directly accept its results in case of fully autonomous system. More precisely, in the perspective of an industrialized use of AI, we would like to study the role of explainability for stakeholders in the decision-making process as well as in value creation. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",16,10.1007/978-3-031-05760-1_57,"This paper explores the relationship between explainability in AI systems and trust among users, particularly in the context of industrial applications. It aims to understand how explainability influences decision-making and value creation for stakeholders. Since it focuses on the impact of AI on trust and decision-making within an economic context, it aligns with Class A. Class: A",A,1529,67,stop,fp_0ba0d124f1
"Enterprises may often deal with situations in which they cannot use a part of their data for machine learning: (1) Privacy laws grant users to determine that their data should not be used any longer by the data holder (i.e., a data record must be removed). (2) Privacy laws may require anonymization, which leads to the removal and masking of specific attributes from a dataset. (3) To avoid ethical issues, it may be necessary to prevent machine learning algorithms from using attributes in their decision-making that may be viewed as discriminatory (i.e., a feature may not be used because it is unethical). Given these challenges, we analyze how removing data from an information system in the cases mentioned above affects its predictive performance and the fairness of its decisions. © 2021, Springer Nature Switzerland AG.",16,10.1007/978-3-030-87205-2_14,"This paper analyzes the impact of data removal on predictive performance and fairness in machine learning, focusing on scenarios involving privacy laws and ethical considerations. It examines how the absence of certain data attributes affects algorithmic outcomes. Since the paper investigates the implications of data handling on fairness and decision-making within an economic context, it aligns with Class A. Class: A",A,1513,69,stop,fp_0ba0d124f1
"With the increasing use of artificial intelligence (AI) applications in decision making, there are heightened concerns about the fairness of such decisions. Initiatives like Responsible AI, Fair ML, Ethics in AI have provided guidelines for developing AI as an attempt to address these challenges. These approaches have been criticized for taking a top down approach by applying abstract principles to practice without taking into account the context and particularities of the algorithm development. Using the sociotechnical lens, we propose a framework for developing Fair algorithm. We apply this framework to mitigate unfairness in three distinct datasets: COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), Crimes and Community, and a synthetic dataset. Our methodology involves nonconvex optimization for regression with fairness constraints. The experimentation examines the correlation coefficient, Area Under the Curve (AUC), and Root Mean Square Error (RMSE) in relation to a fairness parameter, epsilon. Our findings suggest three objectively testable propositions namely, 1) Fairness Constraint and Predictive power, 2) Fairness Constraints and Discriminatory Ability, 3) Fairness Constraints and Prediction Accuracy. © 2024",16,10.1016/j.jjimei.2024.100259,This paper addresses concerns about fairness in AI decision-making by proposing a sociotechnical framework for developing fair algorithms. It applies this framework to various datasets and employs nonconvex optimization to analyze the relationship between fairness constraints and predictive performance. The focus on fairness in AI decision-making within an economic context aligns it with Class A. Class: A,A,1580,67,stop,fp_0ba0d124f1
"This study proposes a real-time Decision Support System (DSS) using machine learning to enhance proactive management of Human–Machine Interaction (HMI) in safety–critical digital control rooms. The DSS provides explainable predictions and recommendations regarding near-future automation usage, customized for the railway control room management, who supervise the operations of traffic controllers (TCs). In this setting, TCs decide on the spot whether to manually or automatically open signals to regulate railway traffic, a critical aspect of ensuring punctuality and safety. This time-setting specific HMI differs across TCs and is not yet supported by a data-driven tool. The proposed DSS includes agreement levels for predictions among different modeling paradigms: linear models, tree-based models, and deep neural networks. SHAP (SHapley Additive exPlanations) values are deployed to assess the agreement level in explainability between these different modeling paradigms. The prescriptions are based on the HMI of well-performing peers. We implement the DSS as proof of concept at the Belgian railway infrastructure company and report end-user feedback on the perception, the operational impact, and the inclusion of agreement levels. © 2024 Elsevier B.V.",16,10.1016/j.dss.2024.114216,"This study develops a real-time Decision Support System (DSS) using machine learning to improve Human-Machine Interaction (HMI) in railway control rooms. It focuses on providing explainable predictions and recommendations for traffic controllers to enhance safety and punctuality. The methodology includes various modeling paradigms and SHAP values for explainability assessment. Since the paper emphasizes the development of a tool for decision support rather than analyzing the impacts of AI on fairness or ethical outcomes, it should be classified as Class B. Class: B",B,1587,102,stop,fp_0ba0d124f1
"Artificial intelligence (AI) is a major contributor in industry 4.0 and there exists a strong push for AI adoption across fields for both research and practice. However, AI has quite well elaborated risks for both business and general society. Hence, paying attention to avoiding hurried adoption of counter-productive practices is important. For both managerial and general social issues, the same solution is sometimes proposed: human-in-the-loop (HITL). However, HITL literature is contradictory: HITL is proposed to promote fairness, accountability, and transparency of AI, which are sometimes assumed to come at the cost of AI accuracy. Yet, HITL is also considered a way to improve accuracy. To make sense of the convoluted literature, we begin to explore qualitatively how explainability is constructed in a HITL process, and how method accuracy is affected as its function. To do this, we study qualitatively and quantitatively a multi-class classification task with multiple machine learning algorithms. We find that HITL can increase both accuracy and explainability, but not without deliberate effort to do so. The effort required to achieve both increased accuracy and explainability, requires an iterative HITL in which accuracy improvements are not continuous, but disrupted by unique and varying human biases shedding additional perspectives on the task at hand. © 2022 IEEE.",16,10.1109/ICE/ITMC-IAMOT55089.2022.10033225,"This paper explores the role of human-in-the-loop (HITL) processes in artificial intelligence, focusing on how they can enhance both explainability and accuracy in machine learning tasks. It examines the contradictory literature surrounding HITL and its implications for fairness, accountability, and transparency. The study employs qualitative and quantitative methods to analyze the effects of HITL on algorithm performance. Since it investigates the impacts of AI on fairness and decision-making within an economic context, it aligns with Class A. Class: A",A,1616,99,stop,fp_0ba0d124f1
"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law's practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of 'auditor roles' that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.  © 2024 Owner/Author.",16,10.1145/3630106.3658959,"This paper analyzes New York City's Local Law 144, which mandates annual bias audits for automated employment decision-making tools. Through qualitative interviews with experts, it identifies shortcomings in the law's implementation, such as vague definitions and industry lobbying that undermine its effectiveness. The focus on auditing as an accountability mechanism in the context of algorithmic bias aligns with ethical outcomes in an economic context. Class: A",A,1705,77,stop,fp_0ba0d124f1
"Despite the advancement of algorithm-based AI transforming business and society, there is growing evidence of service failures caused by algorithmic mistakes. Due to the “black box” nature of algorithmic decisions, consumers are frustrated not only by the mistakes themselves but also by the lack of interpretability of algorithmic decisions. Thus, the current research focuses on the impact of enhanced algorithmic interpretability through Explainable Artificial Intelligence (XAI) approaches (e.g., post-hoc explanations) on consumer reactions to service failures resulting from algorithmic mistakes. Across four experimental studies, the authors demonstrate that consumers react less negatively to service failures caused by algorithmic (rather than human) mistakes when algorithmic interpretability is enhanced. This effect is primarily due to reduced blame assigned to algorithms. Furthermore, they show that the beneficial effect disappears when algorithms are employed for an objective (vs. a subjective) task and when algorithms are at a weak (vs. strong) intelligence stage. © 2024 Elsevier Inc.",16,10.1016/j.jbusres.2024.114610,This paper examines how enhanced interpretability of algorithmic decisions through Explainable Artificial Intelligence (XAI) affects consumer reactions to service failures caused by algorithms. It utilizes experimental studies to demonstrate that improved interpretability can mitigate negative consumer responses by reducing blame assigned to algorithms. The focus on consumer reactions and the implications of AI decisions within an economic context aligns it with Class A. Class: A,A,1549,76,stop,fp_0ba0d124f1
"Rapid developments in artificial intelligence (AI) technologies have brought intelligent decision-making applications into various fields, significantly facilitating our societal and economic lives. However, with the deeper integration of AI-enabled system decision making in related industries, accidents also happen frequently. It has been found that AI-enabled systems, while superhuman in certain abilities, are subject to endogenous risks such as black box, bias, security, and unaccountability; people also have trust concerns when interacting with AI-enabled systems. These together lead to a crisis of trust in AI-enabled system decision-making. Especially in high-risk scenarios, wrong predictions and bad decisions of AI-enabled systems will lead to unbearable consequences for people. In order to cope with the abovementioned risks and uncertainties, trustworthy decision making for AI-enabled systems is widely discussed in academia and has become a hot research topic of great interest. This paper systematically reviews the research progress and challenges of trustworthy decision making for AI-enabled systems, including its risk sources, method application research and its limitations, and proposes a collaborative human-machine trustworthy framework throughout the AI development lifecycle, and provides an outlook of future research. This paper first analyzes the risk sources affecting the trustworthy decision making of AI-enabled systems and finds that the risks mainly originate from two aspects, namely, the endogenous risk of itself and the trust problem in human-computer interaction, by summarizing the existing research on the application of AI-enabled systems. The former is mainly the model perspective and data-driven endogenous risk, including the unexplainable risk and unaccountable risk caused by the black-box characteristics of AI models, as well as the unfair risk caused by data bias and unreliability risk caused by data security, etc. The latter is the trust risk arising from the need to entrust AI to make decisions unilaterally or adopt human-machine collaborative decision making, and these above-mentioned risks seriously affect users' trust in AI system adoption and decision making. Next, this paper focuses on the research progress related to trustworthy decision making of AI-enabled systems from the perspective of risk sources of AI-enabled systems, including models, data, and trust. Specifically, we analyze the research progress from the trend of literature on trusted AI research to the interpretability and accountability of AI models, from fairness of big data decision making to data security and summarize the research progress from the perspective of risk sources, ethical guidelines, and industry applications to make a preliminary analysis of industry heterogeneity and common patterns of trusted AI. Through the analysis of related literature in this paper, it is clear that the research on the academic issues of trustworthy decision making for AI-enabled systems is still in its infancy: the priority relationships and internal boundaries between various ethical principles are not clearly delineated and defined, and the attention to the heterogeneity and common laws of AI decision making in different risk levels and application industries is insufficient, and there are many issues worth exploring. Then, this paper analyzes the challenges of credible decision making in multiple uncertainty contexts under a management perspective, including continuous trust, multiple uncertainties, and the construction of human-machine teams, and then proposes a guiding human-machine collaborative credible framework. From the management perspective, the trustworthy decision making of AIenabled systems is a continuous organic holistic system covering the whole lifecycle of AI-enabled systems, which needs to simultaneously satisfy interpretability, fairness, security privacy, and accountability, also to gain trust in human-machine interaction in order to effectively enhance the trustworthiness of AI-enabled system applications and decisions. Finally, this paper provides an outlook on trustworthy decision-making for AI-enabled systems, including trustworthy humancomputer collaboration team formation, cross-organizational trusted decision-making problems, trusted AI regulatory challenges, and metrics and standards for AI trusted decision-making. In the future, the research on trustworthy decision making of of AI-enabled systems will be more in-depth, thus providing more theoretical and practical values for the development of human society. © 2022, Journal of Industrial Engineering and Engineering Management",16,10.13587/j.cnki.jieem.2022.06.001,"This paper reviews the challenges and progress in achieving trustworthy decision-making for AI-enabled systems, focusing on risks such as bias, unaccountability, and trust issues in human-computer interactions. It analyzes literature on ethical guidelines, model interpretability, and fairness, proposing a collaborative framework for trustworthy AI throughout its lifecycle. The emphasis on the implications of AI for trust and ethical outcomes within an economic context aligns it with Class A. Class: A",A,2134,87,stop,fp_0ba0d124f1
"The Evidence-Based Policy Movement (EBPM) advocates the intensive use of research and data to improve public policies and regulations. The last two decades saw the steady rise of the Big Data Era - a data-driven culture and mass production of information that could be an asset to the EBPM. However, the best models are complex black boxes and lack interpretability. Explainable Artificial Intelligence (XAI) is a booming field that brings explainability to those complex models and could fulfill Evidence-Based Policy needs. Shapley Additive Explanation (SHAP) proposes a unified framework of six previous XAI models and promises to bridge the gap between accuracy and interpretability. The Brazilian Electricity Regulatory Agency (ANEEL) aims at better policymaking by using consumer satisfaction as a regulatory tool. ANEEL uses IASC, a consumer satisfaction index, as a quality indicator that impacts companies' price cap and profit margins. IASC intends to emulate competition in a monopolist market and ensure the consumer is a significant part of companies' strategic objectives. To maximize the index's potential and engage economic actors in the pursuit of consumer satisfaction drivers, ANEEL must increase the model's predictive power and identify the most relevant features that will help achieve better results and cost-benefit. We propose a new model using Recurrent Neural Network and SHAP to accomplish this goal and keep transparency while reaching higher accuracy, producing actionable insights. We then compared this new model with the current model that uses structural equation modeling, analyzed caveats in employing SHAP, and assessed its benefits and advantages.  © 2021 IEEE.",16,10.1109/CBI52690.2021.10055,"This paper discusses the integration of Explainable Artificial Intelligence (XAI) into the Evidence-Based Policy Movement (EBPM) to enhance public policymaking, specifically in the context of the Brazilian Electricity Regulatory Agency. It proposes a new model using Recurrent Neural Networks and SHAP to improve predictive power and transparency in consumer satisfaction metrics. The focus on how XAI can improve decision-making and policy outcomes aligns it with economic implications. Class: A",A,1668,87,stop,fp_0ba0d124f1
"Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users' background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.  © 2024 Owner/Author.",16,10.1145/3630106.3659031,"This paper focuses on the evaluation of explainable AI (XAI) features in the context of disinformation detection, emphasizing the importance of human-centered evaluation. It assesses how different XAI features impact performance, perceived usefulness, understandability, and trust among users with varying expertise. The study highlights the implications of AI transparency and human oversight, particularly regarding fundamental human rights. Since it analyzes the impacts of AI on decision-making and ethical outcomes, it aligns with Class A. Class: A",A,1677,96,stop,fp_0ba0d124f1
"High uncertainty tasks such as making a medical diagnosis, judging a criminal justice case and driving in a big city have a very low margin for error because of the potentially devastating consequences for human lives. In this paper, we focus on how humans learn from uncertainty while performing a high uncertainty task with AI systems. We analyze Tesla's autonomous driving systems (ADS), a type of AI system, drawing on crash investigation reports, published reports on formal simulation tests and YouTube recordings of informal simulation tests by amateur drivers. Our empirical analysis provides insights into how varied levels of uncertainty tolerance have implications for how humans learn from uncertainty in real-time and over time to jointly perform the driving task with Tesla's ADS. Our core contribution is a theoretical model that explains human-AI joint task performance. Specifically, we show that, the interdependencies between different modes of AI use including uncontrolled automation, limited automation, expanded automation, and controlled automation are dynamically shaped through humans' learning from uncertainty. We discuss how humans move between these modes of AI use by increasing, reducing, or reinforcing their uncertainty tolerance. We conclude by discussing implications for the design of AI systems, policy into delegation in joint task performance, as well as the use of data to improve learning from uncertainty. © 2024 The Authors",16,10.1016/j.infoandorg.2024.100502,"This paper analyzes human learning from uncertainty in high-stakes tasks while interacting with AI systems, specifically focusing on Tesla's autonomous driving technology. It develops a theoretical model to explain the dynamics of human-AI joint task performance and discusses implications for AI system design and policy. The focus on how AI impacts decision-making and human behavior in critical contexts aligns it with economic outcomes. Class: A",A,1603,76,stop,fp_0ba0d124f1
"AI decisions are increasingly determining our everyday lives. At present, European anti-discrimination law is process-oriented; it prohibits the inclusion of sensitive data that is particularly protected. However, especially in the context of AI decisions, constellations can be identified in which the inclusion of sensitive characteristics will lead to better and sometimes even less discriminatory result. A result-oriented approach, therefore, might be a more fitting strategy for algorithmic decision making. In this paper we examine the legal framework for including sensitive features in a Support Vector Machine for a fictitious scenario and discuss the resulting challenges in practical application. It turns out that generally ignoring sensitive features - as has been the practice up to now - does not seem to be a fitting strategy for algorithmic decision making. A process-oriented procedure only supposedly comes closer to individual case justice: If one assumes that fewer errors occur when protected characteristics are included, individuals will ultimately also be assessed incorrectly less often, especially when one protected group is more prone to errors than the other. This paper aims to support the current debate about legal regulation of algorithmic decision making systems by discussing an often neglected perspective. © 2022 Hanna Hoffmann, Verena Vogt, Marc P. Hauer, Katharina Zweig",16,10.1016/j.clsr.2022.105658,"This paper critiques the current process-oriented approach of European anti-discrimination law in the context of AI decision-making. It argues for a result-oriented approach that allows the inclusion of sensitive characteristics in algorithms to potentially reduce discrimination. The focus is on the legal implications and practical challenges of this approach, linking it to fairness and ethical outcomes in algorithmic decisions. Thus, it aligns with Class A. Class: A",A,1597,80,stop,fp_0ba0d124f1
"Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences. © 2021 Owner/Author.",16,10.1145/3442188.3445873,"This paper evaluates the use of machine learning in government resource allocation decisions, specifically focusing on environmental enforcement by the EPA. It highlights how algorithmic design choices can lead to disparate impacts on minority populations and discusses the implications of classification versus regression models in addressing compliance. The analysis of these impacts within an economic context aligns with Class A. Class: A",A,1578,68,stop,fp_0ba0d124f1
"Should firms that apply machine learning algorithms in their decision making make their algorithms transparent to the users they affect? Despite the growing calls for algorithmic transparency, most firms keep their algorithms opaque, citing potential gaming by users that may negatively affect the algorithm’s predictive power. In this paper, we develop an analytical model to compare firm and user surplus with and without algorithmic transparency in the presence of strategic users and present novel insights. We identify a broad set of conditions under which making the algorithm transparent actually benefits the firm. We show that, in some cases, even the predictive power of the algorithm can increase if the firm makes the algorithm transparent. By contrast, users may not always be better off under algorithmic transparency. These results hold even when the predictive power of the opaque algorithm comes largely from correlational features and the cost for users to improve them is minimal. We show that these insights are robust under several extensions of the main model. Overall, our results show that firms should not always view manipulation by users as bad. Rather, they should use algorithmic transparency as a lever to motivate users to invest in more desirable features. © 2023 INFORMS Inst.for Operations Res.and the Management Sciences. All rights reserved.",16,10.1287/mnsc.2022.4475,"This paper analyzes the implications of algorithmic transparency for firms using machine learning in decision-making. It develops an analytical model to compare firm and user surplus under different transparency conditions, revealing that transparency can benefit firms and potentially enhance predictive power. The focus on the economic outcomes of algorithmic transparency aligns it with Class A. Class: A",A,1595,65,stop,fp_0ba0d124f1
"Many important decisions are increasingly being made with the help of information systems that use artificial intelligence and machine learning models. These computational models are designed to discover useful patterns from large amounts of data, which augment human capabilities to make decisions in various application domains. However, there are growing concerns regarding the ethics challenges faced by these automated decision-making (ADM) models, most notably on the issue of algorithmic bias, in which the models systematically produce less favorable (i.e., unfair) decisions for certain groups of people. In this commentary, we argue that algorithmic bias is not just a technical (e.g., computational or statistical) problem, and its successful resolution requires deep insights into individual and organizational behavior, economic incentives, as well as complex dynamics of the sociotechnical systems in which the ADM models are embedded. We discuss a human-centric, fairness-aware ADM framework that highlights the holistic involvement of human decision makers in each step of ADM. We review the emerging literature on fairness-aware machine learning and then discuss various strategic decisions that humans need to make, such as formulating proper fairness objectives, recognizing fairness-induced trade-offs and implications, utilizing machine learning model outputs, and managing/governing the decisions of ADM models. We further illustrate how these strategic decisions are jointly informed by behavioral, economic, and design sciences. Our discussions reveal a number of future research opportunities uniquely suitable for Management Information Systems (MIS) researchers to pursue. © 2022 Association for Computing Machinery. All rights reserved.",16,10.1145/3519420,"This commentary addresses the ethical challenges of algorithmic bias in automated decision-making (ADM) systems, emphasizing that it is not merely a technical issue but one that involves human behavior and economic incentives. It proposes a human-centric framework for fairness-aware ADM, discussing the strategic decisions necessary for managing bias. The focus on the implications of AI for fairness and decision-making within an economic context aligns it with Class A. Class: A",A,1646,83,stop,fp_0ba0d124f1
"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development lifecycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity. © 2020 Copyright held by the owner/author(s).",16,10.1145/3351095.3372873,"This paper presents a framework for algorithmic auditing aimed at identifying and mitigating harmful repercussions of AI systems throughout their development lifecycle. It emphasizes the importance of internal audits based on organizational values to enhance accountability in AI deployment. The focus is on creating a structured process for auditing rather than analyzing the impacts of AI on fairness or ethical outcomes. Therefore, it should be classified as Class B. Class: B",B,1545,78,stop,fp_0ba0d124f1
"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the signiicance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-speciic model information can calibrate trust and improve the joint performance of the human and AI. Speciically, we study the efect of showing conidence score and local explanation for a particular prediction. Through two human experiments, we show that conidence score can help calibrate people's trust in an AI model, but trust calibration alone is not suicient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI. © 2020 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.",16,10.1145/3351095.3372852,"This paper focuses on AI-assisted decision-making, emphasizing the importance of calibrating human trust in AI systems to optimize outcomes. It conducts case studies and experiments to assess how features like confidence scores and local explanations can influence trust and decision performance. The analysis of trust calibration and its impact on decision-making outcomes aligns with ethical considerations in AI, making it relevant to economic contexts. Class: A",A,1683,76,stop,fp_0ba0d124f1
"Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff. © 2019 Association for Computing Machinery.",16,10.1145/3287560.3287590,"This paper explores the interaction between machine learning models and human decision-making in critical tasks, focusing on how explanations and predictions from these models can enhance human performance while maintaining agency. It investigates the balance between human agency and machine assistance, using deception detection as a case study. The analysis of how machine learning impacts ethical decision-making aligns with economic implications. Class: A",A,1614,71,stop,fp_0ba0d124f1
"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools aect actual decision-making processes. After all, risk assessments do not make denitive decisions-they inform judges, who are the nal arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a rst step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed ecacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not eectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with “disparate interactions,” whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new “algorithm-in-the-loop” framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential. © 2019 Copyright held by the owner/author(s).",16,10.1145/3287560.3287563,"This paper examines the impact of risk assessment tools in the U.S. criminal justice system, focusing on how judges interpret and use these tools in decision-making. Through a controlled experiment, it highlights issues of efficacy and fairness, including biases in risk predictions based on race. The study advocates for an ""algorithm-in-the-loop"" framework to better integrate machine learning with human decision-making. Given its focus on the implications of AI on fairness and decision-making in an economic context (criminal justice), it aligns with Class A. Class: A",A,1639,105,stop,fp_0ba0d124f1
"Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",16,10.1080/12460125.2020.1819094,"This paper examines the relationship between transparency in AI decision-making and human trust in assistive AI systems. Through a behavioral experiment, it investigates how varying levels of information affect trust in AI predictions, revealing that increased transparency can sometimes negatively impact trust. The focus on the implications of AI for decision-making and trust aligns it with economic outcomes. Class: A",A,1535,69,stop,fp_0ba0d124f1
"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness""in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies. © 2021 ACM.",16,10.1145/3442188.3445928,"This paper outlines a framework for algorithmic auditing, focusing on the case study of pymetrics, a startup using machine learning for job candidate recommendations. It discusses the complexities of ensuring fairness in algorithmic systems, including social, legal, and technical aspects, and presents findings from an independent audit of pymetrics' software. The emphasis is on practical recommendations for conducting audits to enhance accountability and fairness in algorithmic decision-making. Since it primarily addresses the auditing process rather than analyzing the impacts of AI on fairness, it should be classified as Class B. Class: B",B,1591,111,stop,fp_0ba0d124f1
"Artificial intelligence algorithms are increasingly adopted as decisional aides by public bodies, with the promise of overcoming biases of human decision-makers. At the same time, they may introduce new biases in the human-algorithm interaction. Drawing on psychology and public administration literatures, we investigate two key biases: overreliance on algorithmic advice even in the face of “warning signals” from other sources (automation bias), and selective adoption of algorithmic advice when this corresponds to stereotypes (selective adherence). We assess these via three experimental studies conducted in the Netherlands: In study 1 (N = 605), we test automation bias by exploring participants' adherence to an algorithmic prediction compared to an equivalent human-expert prediction. We do not find evidence for automation bias. In study 2 (N = 904), we replicate these findings, and also test selective adherence. We find a stronger propensity for adherence when the advice is aligned with group stereotypes, with no significant differences between algorithmic and human-expert advice. In study 3 (N = 1,345), we replicate our design with a sample of civil servants. This study was conducted shortly after a major scandal involving public authorities' reliance on an algorithm with discriminatory outcomes (the “childcare benefits scandal”). The scandal is itself illustrative of our theory and patterns diagnosed empirically in our experiment, yet in our study 3, while supporting our prior findings as to automation bias, we do not find patterns of selective adherence. We suggest this is driven by bureaucrats' enhanced awareness of discrimination and algorithmic biases in the aftermath of the scandal. We discuss the implications of our findings for public sector decision making in the age of automation. Overall, our study speaks to potential negative effects of automation of the administrative state for already vulnerable and disadvantaged citizens. © The Author(s) 2022.",16,10.1093/jopart/muac007,"This paper investigates biases in human-algorithm interactions within public decision-making, focusing on automation bias and selective adherence to algorithmic advice. It employs three experimental studies to assess these biases among participants and civil servants, particularly in the context of a recent scandal involving algorithmic discrimination. The findings highlight the implications of algorithmic decision-making for vulnerable populations. Given its focus on the impacts of AI on fairness and ethical outcomes in an economic context, it aligns with Class A. Class: A",A,1721,96,stop,fp_0ba0d124f1
"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of ε perturbations to a fairness parameter ε in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of “fairness-to-welfare” solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring “more fair” classifiers does not abide by the Pareto Principle-a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",16,10.1145/3351095.3372857,"This paper analyzes the relationship between fairness in machine learning and social welfare, focusing on the welfare impacts of fairness-constrained algorithms. It characterizes how variations in fairness parameters affect utility outcomes for individuals and groups, and critiques the effectiveness of strict fairness criteria in improving welfare. The emphasis on the implications of AI for social welfare aligns it with economic outcomes. Class: A",A,1671,72,stop,fp_0ba0d124f1
"Artificial intelligence is similar to human intelligence, and robots in organisations always perform human tasks. However, AI encounters a variety of biases during its operational process in the online economy. The coded algorithms helps in decision-making in firms with a variety of biases and ambiguity. The study is qualitative in nature and asserts that AI biases and vulnerabilities experienced by people across industries lead to gender biases and racial discrimination. Furthermore, the study describes the different types of biases and emphasises the importance of responsible AI in firms in order to reduce the risk from AI. The implications discuss how policymakers, managers, and employees must understand biases to improve corporate fairness and societal well-being. Future research can be carryout on consumer bias, bias in job automation and bias in societal data. © 2023 The Author(s)",16,10.1016/j.jjimei.2023.100165,"This paper qualitatively examines biases in artificial intelligence within organizations, focusing on how these biases can lead to gender and racial discrimination. It emphasizes the need for responsible AI to mitigate these biases and discusses implications for policymakers and managers to enhance corporate fairness and societal well-being. The analysis of AI biases and their impact on ethical outcomes aligns it with economic contexts. Class: A",A,1507,72,stop,fp_0ba0d124f1
"Algorithms based on Artificial Intelligence technologies are slowly transforming street-level bureaucracies, yet a lack of algorithmic transparency may jeopardize citizen trust. Based on procedural fairness theory, this article hypothesizes that two core elements of algorithmic transparency (accessibility and explainability) are crucial to strengthening the perceived trustworthiness of street-level decision-making. This is tested in one experimental scenario with low discretion (a denied visa application) and one scenario with high discretion (a suspicion of welfare fraud). The results show that: (1) explainability has a more pronounced effect on trust than the accessibility of the algorithm; (2) the effect of algorithmic transparency not only pertains to trust in the algorithm itself but also—partially—to trust in the human decision-maker; (3) the effects of algorithmic transparency are not robust across decision context. These findings imply that transparency-as-accessibility is insufficient to foster citizen trust. Algorithmic explainability must be addressed to maintain and foster trustworthiness algorithmic decision-making. © 2022 The Author. Public Administration Review published by Wiley Periodicals LLC on behalf of American Society for Public Administration.",16,10.1111/puar.13483,"This paper examines the relationship between algorithmic transparency and citizen trust in street-level bureaucracies, focusing on the roles of accessibility and explainability. It tests these hypotheses through experimental scenarios involving decision-making contexts. The findings highlight the importance of explainability in fostering trust, indicating that transparency alone is insufficient. Since the paper analyzes the impacts of AI on trust and decision-making within an economic context, it aligns with Class A. Class: A",A,1576,86,stop,fp_0ba0d124f1
"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making. © 2021 ACM.",16,10.1145/3442188.3445917,"This paper analyzes the intersection of explainable AI and legal requirements under the EU data protection framework, particularly focusing on the explainability of AI in high-risk applications like medical diagnosis. It discusses the challenges of providing adequate explanations for complex machine learning models and proposes a multifaceted approach to explanations based on context. The emphasis on legal implications and ethical considerations in AI aligns it with the analysis of fairness and decision-making outcomes. Class: A",A,1790,85,stop,fp_0ba0d124f1
"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines. © 2021 ACM.",16,10.1145/3442188.3445912,This paper addresses the challenge of creating fair and equitable decision-making systems that utilize machine learning and mechanism design. It emphasizes the need for an integrated framework that combines insights from both fields to tackle fairness and discrimination in complex decision-making contexts. The focus on fairness in algorithmic decision-making within an economic context aligns it with Class A. Class: A,A,1586,68,stop,fp_0ba0d124f1
"We present a formal approach to build and evaluate AI systems that include principles of Fairness, Accountability and Transparency (FAT), which are extremely important in various domains where AI models are used, yet their utilization in business settings is scant. We develop and instantiate a FAT-based framework with a privacy-constrained dataset and build a model to demonstrate the balance among these 3 dimensions. These principles are gaining prominence with higher awareness of privacy and fairness in business and society. Our results indicate that FAT can co-exist in a well-designed system. Our contribution lies in presenting and evaluating a functional, FAT-based machine learning model in an affinity prediction scenario. Contrary to common belief, we show that explainable AI/ML systems need not have a major negative impact on predictive performance. Our approach is applicable in a variety of fields such as insurance, health diagnostics, government funds allocation and other business settings. Our work has broad policy implications as well, by making AI and AI-based decisions more ethical, less controversial, and hence, trustworthy. Our work contributes to emerging AI policy perspectives worldwide. © 2021 Elsevier B.V.",16,10.1016/j.dss.2021.113715,"This paper develops a framework for integrating Fairness, Accountability, and Transparency (FAT) into AI systems, particularly in business contexts. It evaluates a machine learning model that adheres to these principles, demonstrating that ethical AI can coexist with predictive performance. The focus on ethical implications and policy contributions aligns it with economic outcomes related to AI deployment. Class: A",A,1574,71,stop,fp_0ba0d124f1
"Data provenance, a record that describes the origins and processing of data, offers new promises in the increasingly important role of artificial intelligence (AI)-based systems in guiding human decision making. To avoid disastrous outcomes that can result from bias-laden AI systems, responsible AI builds on four important characteristics: fairness, accountability, transparency, and explainability. To stimulate further research on data provenance that enables responsible AI, this study outlines existing biases and discusses possible implementations of data provenance to mitigate them. We first review biases stemming from the data's origins and pre-processing. We then discuss the current state of practice, the challenges it presents, and corresponding recommendations to address them. We present a summary highlighting how our recommendations can help establish data provenance and thereby mitigate biases stemming from the data's origins and pre-processing to realize responsible AI-based systems. We conclude with a research agenda suggesting further research avenues. © 2022 Copyright held by the owner/author(s).",16,10.1145/3503488,"This paper focuses on the role of data provenance in promoting responsible AI by addressing biases in data origins and processing. It reviews existing biases and discusses implementations of data provenance to mitigate these issues, while also outlining a research agenda for future studies. Since it emphasizes methods for mitigating bias rather than analyzing the impacts of AI on fairness or ethical outcomes, it should be classified as Class B. Class: B",B,1539,78,stop,fp_0ba0d124f1
"Abstract. Because of a growing number of initiatives and regulations, predictions of modern artificial intelligence (AI) systems increasingly come with explanations about why they behave the way they do. In this paper, we explore the impact of feature-based explanations on users' information processing. We designed two complementary empirical studies where participants either made incentivized decisions on their own, with the aid of opaque predictions, or with explained predictions. In Study 1, laypeople engaged in the deliberately abstract investment game task. In Study 2, experts from the real estate industry estimated listing prices for real German apartments. Our results indicate that the provision of featurebased explanations paves the way for AI systems to reshape users' sense making of information and understanding of the world around them. Specifically, explanations change users' situational weighting of available information and evoke mental model adjustments. Crucially, mental model adjustments are subject to the confirmation bias so that misconceptions can persist and even accumulate, possibly leading to suboptimal or biased decisions. Additionally, mental model adjustments create spillover effects that alter user behavior in related yet disparate domains. Overall, this paper provides important insights into potential downstream consequences of the broad employment of modern explainable AI methods. In particular, side effects of mental model adjustments present a potential risk of manipulating user behavior, promoting discriminatory inclinations, and increasing noise in decision making. Our findings may inform the refinement of current efforts of companies building AI systems and regulators that aim to mitigate problems associated with the black-box nature of many modern AI systems. © 2023 The Author(s).",16,10.1287/isre.2023.1199,"This paper examines the effects of feature-based explanations in AI on users' decision-making processes through two empirical studies involving laypeople and industry experts. It highlights how explanations can reshape users' understanding and lead to confirmation bias, potentially resulting in biased or suboptimal decisions. The focus on the implications of AI explanations for decision-making and ethical outcomes aligns it with economic contexts. Class: A",A,1660,75,stop,fp_0ba0d124f1
"Ensuring fairness in algorithmic decision making is a crucial policy issue. Current legislation ensures fairness by barring algorithm designers fromusing demographic information in their decision making. As a result, to be legally compliant, the algorithms need to ensure equal treatment. However, in many cases, ensuring equal treatment leads to disparate impact particularly when there are differences among groups based on demographic classes. In response, several ""fair""machine learning (ML) algorithms that require impact parity (e.g., equal opportunity) at the cost of equal treatment have recently been proposed to adjust for the societal inequalities. Advocates of fair ML propose changing the law to allow the use of protected class-specific decision rules.We show that the proposed fairML algorithms that require impact parity, while conceptually appealing, can make everyone worse off, including the very class they aim to protect. Compared with the current law, which requires treatment parity, the fairML algorithms, which require impact parity, limit the benefits of a more accurate algorithm for a firm. As a result, profit maximizing firms could underinvest in learning, that is, improving the accuracy of their machine learning algorithms. We show that the investment in learning decreases when misclassification is costly, which is exactly the case when greater accuracy is otherwise desired. Our paper highlights the importance of considering strategic behavior of stake holders when developing and evaluating fair ML algorithms. Overall, our results indicate that fair ML algorithms that require impact parity, if turned into law,may not be able to deliver some of the anticipated benefits. © 2021 INFORMS.",16,10.1287/mnsc.2021.4065,"This paper analyzes the implications of fairness in algorithmic decision-making, particularly the tension between equal treatment and impact parity in machine learning algorithms. It critiques proposed fair ML algorithms, arguing that they may lead to worse outcomes for all, including the groups they aim to protect, by limiting firms' incentives to improve algorithm accuracy. The focus on the economic consequences of fairness in algorithm design aligns it with Class A. Class: A",A,1661,83,stop,fp_0ba0d124f1
"We consider two fundamental and related issues currently facing the development of Artificial Intelligence (AI): the lack of ethics, and the interpretability of AI decisions. Can interpretable AI decisions help to address the issue of ethics in AI? Using a randomized study, we experimentally show that the empirical and liberal turn of the production of explanations tends to select AI explanations with a low denunciatory power. Under certain conditions, interpretability tools are therefore not means but, paradoxically, obstacles to the production of ethical AI since they can give the illusion of being sensitive to ethical incidents. We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education of the person for whom the explication is intended. AI ethics tools are therefore sometimes too flexible and self-regulation through the liberal production of explanations does not seem to be enough to resolve ethical issues. By following an STS pragmatist program, we highlight the role of non-human actors (such as computational paradigms, testing environments, etc.) in the formation of structural power relations, such as sexism. We then propose two scenarios for the future development of ethical AI: more external regulation, or more liberalization of AI explanations. These two opposite paths will play a major role in the future development of ethical AI. © 2021",16,10.1016/j.techfore.2021.121209,"This paper explores the relationship between interpretability in AI and ethical decision-making. It uses a randomized study to assess how AI explanations can sometimes hinder ethical outcomes by providing misleading reassurances. The analysis highlights the contextual factors affecting the effectiveness of AI explanations and discusses the implications for future AI regulation. Since it critically analyzes the ethical implications of AI within an economic context, it aligns with Class A. Class: A",A,1621,80,stop,fp_0ba0d124f1
"Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them-and who is not-are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions. © 2020 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.",16,10.1145/3351095.3375784,"This paper examines the use of AI in targeted social policies for poverty alleviation, focusing on the accuracy of eligibility algorithms for cash transfers and other benefits. It highlights the potential for AI to improve coverage of the poor while also addressing disparities that may arise without fairness constraints. The study emphasizes the need for a decision-support platform to incorporate diverse fairness criteria in AI applications. Given its focus on the implications of AI for fairness and social outcomes in an economic context, it aligns with Class A. Class: A",A,1612,99,stop,fp_0ba0d124f1
"Artificial Intelligence is increasingly used to support and improve street-level decision-making, but empirical evidence on how street-level bureaucrats' work is affected by AI technologies is scarce. We investigate how AI recommendations affect street-level bureaucrats' decision-making and if explainable AI increases trust in such recommendations. We experimentally tested a realistic mock predictive policing system in a sample of Dutch police officers using a 2 × 2 factorial design. We found that police officers trust and follow AI recommendations that are congruent with their intuitive professional judgment. We found no effect of explanations on trust in AI recommendations. We conclude that police officers do not blindly trust AI technologies, but follow AI recommendations that confirm what they already thought. This highlights the potential of street-level discretion in correcting faulty AI recommendations on the one hand, but, on the other hand, poses serious limits to the hope that fair AI systems can correct human biases. © 2023 The Authors. Public Administration Review published by Wiley Periodicals LLC on behalf of American Society for Public Administration.",16,10.1111/puar.13602,"This paper examines the impact of AI recommendations on street-level bureaucrats' decision-making, specifically focusing on police officers' trust in AI systems. It employs an experimental design to assess how congruence with professional judgment influences trust and adherence to AI recommendations. The findings highlight the limitations of AI in correcting human biases, emphasizing the role of human discretion. Since it analyzes the implications of AI on decision-making and fairness in an economic context (policing), it aligns with Class A. Class: A",A,1554,98,stop,fp_0ba0d124f1
"Whereas using artificial intelligence (AI) to predict natural hazards is promising, applying a predictive policing algorithm (PPA) to predict human threats to others continues to be debated. Whereas PPAs were reported to be initially successful in Germany and Japan, the killing of Black Americans by police in the US has sparked a call to dismantle AI in law enforcement. However, although PPAs may statistically associate suspects with economically disadvantaged classes and ethnic minorities, the targeted groups they aim to protect are often vulnerable populations as well (e.g., victims of human trafficking, kidnapping, domestic violence, or drug abuse). Thus, determining how to enhance the benefits of PPA while reducing bias through better management is important. In this paper, we propose a policy schema to address this issue. First, after clarifying relevant concepts, we examine major criticisms of PPAs and argue that some of them should be addressed. If banning AI or making it taboo is an unrealistic solution, we must learn from our errors to improve AI. We next identify additional challenges of PPAs and offer recommendations from a policy viewpoint. We conclude that the employment of PPAs should be merged into broader governance of the social safety net and audited publicly by parliament and civic society so that the unjust social structure that breeds bias can be revised. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.",16,10.1007/s11948-021-00312-x,"This paper discusses the controversial use of predictive policing algorithms (PPAs) in law enforcement, particularly in relation to their impact on economically disadvantaged and minority groups. It critiques existing biases in PPAs and proposes a policy framework to enhance their benefits while mitigating bias. The focus on the implications of AI for fairness and ethical outcomes within an economic context aligns it with Class A. Class: A",A,1625,76,stop,fp_0ba0d124f1
"The related literature and industry press suggest that artificial intelligence (AI)-based decision-making systems may be biased towards gender, which in turn impacts individuals and societies. The information system (IS) field has recognised the rich contribution of AI-based outcomes and their effects; however, there is a lack of IS research on the management of gender bias in AI-based decision-making systems and its adverse effects. Hence, the rising concern about gender bias in AI-based decision-making systems is gaining attention. In particular, there is a need for a better understanding of contributing factors and effective approaches to mitigating gender bias in AI-based decision-making systems. Therefore, this study contributes to the existing literature by conducting a Systematic Literature Review (SLR) of the extant literature and presenting a theoretical framework for the management of gender bias in AI-based decision-making systems. The SLR results indicate that the research on gender bias in AI-based decision-making systems is not yet well established, highlighting the great potential for future IS research in this area, as articulated in the paper. Based on this review, we conceptualise gender bias in AI-based decision-making systems as a socio-technical problem and propose a theoretical framework that offers a combination of technological, organisational, and societal approaches as well as four propositions to possibly mitigate the biased effects. Lastly, this paper considers future research on the management of gender bias in AI-based decision-making systems in the organisational context. © 2022 authors. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial 3.0 Australia License, which permits non-commercial use, distribution, and reproduction in any medium, provided the original author and AJIS are credited.",16,10.3127/AJIS.V26I0.3835,"This paper addresses the issue of gender bias in AI-based decision-making systems, highlighting its impact on individuals and society. It conducts a systematic literature review to identify contributing factors and proposes a theoretical framework for managing gender bias through technological, organizational, and societal approaches. The focus on understanding and mitigating bias aligns it with Class B, as it emphasizes methods rather than analyzing the impacts of AI on fairness or ethical outcomes. Class: B",B,1688,84,stop,fp_0ba0d124f1
"Consumers' concerns about how companies gather and use their personal data can impede the widespread adoption of artificial intelligence (AI) technologies. This study demonstrates that mechanistic explanations of AI algorithms can inhibit such data collection concerns. Four independent online experiments show a negative effect of detailed mechanistic explanations on data collection concerns (Studies 1a and 1b), as well as mediating influences of a subjective understanding of how AI algorithms work (Study 2) and increased the likelihood to adopt AI technologies after data collection concerns have been mitigated (Study 3). These findings contribute to research on consumer privacy concerns and the adoption of AI technologies, by identifying (1) a new inhibitor of data collection concerns, namely, mechanistic explanations of AI algorithms; (2) the psychological mechanisms underlying mechanist explanation effects; and (3) how diminished data collection concerns promote AI technology adoption. These insights can help companies design more effective communication strategies that reduce the perceived opacity of AI algorithms, reassure consumers, and encourage their adoption of AI technologies. © 2022 Wiley Periodicals LLC.",16,10.1002/mar.21705,"This paper explores how mechanistic explanations of AI algorithms can alleviate consumer concerns about data collection, thereby promoting the adoption of AI technologies. It employs four online experiments to assess the impact of these explanations on consumer attitudes and behaviors. The focus is on consumer privacy and the implications for AI adoption rather than the ethical outcomes of AI in an economic context. Thus, it should be classified as Class B. Class: B",B,1564,81,stop,fp_0ba0d124f1
"This paper presents a new perspective on the problem of bias in artificial intelligence (AI)-driven decision-making by examining the fundamental difference between AI and human rationality in making sense of data. Current research has focused primarily on software engineers’ bounded rationality and bias in the data fed to algorithms but has neglected the crucial role of algorithmic rationality in producing bias. Using a Weberian distinction between formal and substantive rationality, we inquire why AI-based algorithms lack the ability to display common sense in data interpretation, leading to flawed decisions. We first conduct a rigorous text analysis to uncover and exemplify contextual nuances within the sampled data. We then combine unsupervised and supervised learning, revealing that algorithmic decision-making characterizes and judges data categories mechanically as it operates through the formal rationality of mathematical optimization procedures. Next, using an AI tool, we demonstrate how formal rationality embedded in AI-based algorithms limits its capacity to perform adequately in complex contexts, thus leading to bias and poor decisions. Finally, we delineate the boundary conditions and limitations of leveraging formal rationality to automatize algorithmic decision-making. Our study provides a deeper understanding of the rationality-based causes of AI’s role in bias and poor decisions, even when data is generated in a largely bias-free context. © Association for Information Technology Trust 2023.",16,10.1177/02683962231176842,This paper explores the concept of bias in AI-driven decision-making by contrasting AI's formal rationality with human rationality. It employs text analysis and machine learning techniques to demonstrate how algorithmic decision-making can lead to biased outcomes due to its mechanical nature. The focus on understanding the causes of bias in AI aligns with ethical implications in an economic context. Class: A,A,1616,72,stop,fp_0ba0d124f1
"Artificial Intelligence (AI) has the potential to significantly impact the educational sector. One application of AI that has increasingly been applied is algorithmic grading. It is within this context that our study takes a focus on trust. While the concept of trust continues to grow in importance among AI researchers and practitioners, an investigation of trust/mistrust in algorithmic grading across multiple levels of analysis has so far been under-researched. In this paper, we argue the need for a model that encompasses the multi-layered nature of trust/mistrust in AI. Drawing on an embedded agency perspective, a model is devised that examines top-down and bottom-up forces that can influence trust/mistrust in algorithmic grading. We illustrate how the model can be applied by drawing on the case of the International Baccalaureate (IB) program in 2020, whereby an algorithm was used to determine student grades. This paper contributes to the AI-trust literature by providing a fresh theoretical lens based on institutional theory to investigate the dynamic and multi-faceted nature of trust/mistrust in algorithmic grading—an area that has seldom been explored, both theoretically and empirically. The study raises important implications for algorithmic design and awareness. Algorithms need to be designed in a transparent, fair, and ultimately a trustworthy manner. While an algorithm typically operates like a black box, whereby the underlying mechanisms are not apparent to those impacted by it, the purpose and an understanding of how the algorithm works should be communicated upfront and in a timely manner. © 2022 Elsevier Ltd",16,10.1016/j.ijinfomgt.2022.102555,"This paper focuses on the concept of trust in algorithmic grading within the educational sector, proposing a model that examines the multi-layered nature of trust and mistrust. It emphasizes the importance of transparency and fairness in algorithm design, particularly in the context of the International Baccalaureate program's use of AI for grading. The analysis of trust in AI systems relates to ethical outcomes and decision-making in an economic context. Class: A",A,1661,86,stop,fp_0ba0d124f1
"In this paper we introduce the concept of ‘reviewability' as an alternative approach to improving the accountability of automated decision-making that involves machine learning systems. In doing so, we draw on an understanding of automated decision-making as a socio-technical process, involving both human (organisational) and technical components, beginning before a decision is made and extending beyond the decision itself. Although explanations for automated decisions may be useful in some contexts, they focus more narrowly on the model and therefore do not provide the information about that process as a whole that is necessary for many aspects of accountability, regulatory oversight, and assessments for legal compliance. Drawing on previous work on the application of administrative law and judicial review mechanisms to automated decision-making in the public sector, we argue that breaking down the automated decision-making process into its technical and organisational components allows us to consider how appropriate record-keeping and logging mechanisms implemented at each stage of that process would allow for the process as a whole to be reviewed. Although significant research is needed to explore how it can be implemented, we argue that a reviewability framework potentially offers for a more useful and more holistic form of accountability for automated decision-making than approaches focused more narrowly on explanations. © 2020 Jennifer Cobbe and Jatinder Singh",16,10.1016/j.clsr.2020.105475,"This paper introduces the concept of 'reviewability' to enhance accountability in automated decision-making systems. It critiques existing approaches that focus solely on model explanations and advocates for a comprehensive understanding of the socio-technical process involved. The methodology involves analyzing the decision-making process and proposing mechanisms for better record-keeping and oversight. Since it emphasizes accountability and oversight rather than analyzing the impacts of AI on fairness or ethical outcomes, it should be classified as Class B. Class: B",B,1602,92,stop,fp_0ba0d124f1
"Applying machine learning in real-world applications may have various implications on companies, but individuals as well. Besides obtaining lower costs, faster time to decision and higher accuracy of the decision, automation of decisions can lead to unethical and illegal consequences. More specifically, predictions can systematically discriminate against a certain group of people. This comes mainly due to dataset bias. In this paper, we investigate instances oversampling to improve fairness. We tried several strategies and two techniques, namely SMOTE and random oversampling. Besides traditional oversampling techniques, we tried oversampling of instances based on sensitive attributes as well (i.e. gender or race). We demonstrate on real-world datasets (Adult and COMPAS) that oversampling techniques increase fairness, without greater decrease in predictive accuracy. Oversampling improved fairness up to 15% and AUPRC up to 3% with a loss in AUC of 2%. © 2021, Springer Nature Switzerland AG.",16,10.1007/978-3-030-73976-8_9,"This paper examines the issue of dataset bias in machine learning, focusing on how oversampling techniques can mitigate unfair predictions that discriminate against certain demographic groups. It evaluates the effectiveness of various oversampling strategies, including SMOTE and random oversampling, on real-world datasets. The emphasis is on improving fairness in algorithmic predictions rather than analyzing the broader economic implications of these biases. Therefore, it should be classified as Class B. Class: B",B,1538,86,stop,fp_0ba0d124f1
"Although artificial intelligence can contribute to decision-making processes, many industry players lag behind pioneering companies in utilizing artificial intelligence-driven technologies, which is a significant problem. Explainable artificial intelligence can be a viable solution to mitigate this problem. This paper proposes a research model to address how explainable artificial intelligence can impact decision-making processes. Using an experimental design, empirical data is collected to test the research model. This paper is one of the pioneer papers providing empirical evidence about the impact of explainable artificial intelligence on supply chain decision-making processes. We propose a serial mediation path, which includes transparency and agile decision-making. Findings reveal that explainable artificial intelligence enhances transparency, thereby significantly contributing to agile decision-making for improving cyber resilience during supply chain cyberattacks. Moreover, we conduct a post hoc analysis using text analysis to explore the themes present in tweets discussing explainable artificial intelligence in decision support systems. The results indicate a predominantly positive attitude towards explainable artificial intelligence within these systems. Furthermore, the text analysis reveals two main themes that emphasize the importance of transparency, explainability, and interpretability in explainable artificial intelligence. © 2024 The Authors",16,10.1016/j.dss.2024.114194,"This paper investigates the role of explainable artificial intelligence (AI) in enhancing decision-making processes within supply chains, particularly focusing on transparency and agile responses to cyber threats. It employs an experimental design to collect empirical data and includes a post hoc text analysis of public sentiment towards explainable AI. The emphasis on how AI impacts decision-making aligns it with economic contexts. Class: A",A,1576,74,stop,fp_0ba0d124f1
"Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher efiort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",16,10.1145/3351095.3372842,"This paper examines the sources of unfairness in machine learning models, particularly focusing on market forces and their impact on data investment for different groups. It develops a model to predict unfairness in monopoly and competitive settings, suggesting that competition may not resolve these issues. The analysis emphasizes the need for policy-driven solutions alongside technological interventions to address fairness concerns. Given its focus on the implications of AI on fairness and economic outcomes, it aligns with Class A. Class: A",A,1535,91,stop,fp_0ba0d124f1
"Motivation: The decisions made by modern 'black box' artificial intelligence models are not understandable and therefore people do not trust them. This limits down the potential power of usage of Artificial Intelligence. Aim: The idea of this text is to show the different initiatives in different countries how AI, especially black box AI, can be made transparent and trustworthy and what kind of regulations will be implemented or discussed to be implemented. We also show up how a commonly used development process within Machine Learning can be enriched to fulfil the requirements e.g. of the Ethics guidelines for trustworthy AI of the High-Level Expert Group of the European Union. We support our discussion with a proposition of empirical tools providing interpretability. Results: The full potential of AI or products using AI can only be raised if the decision of AI models are transparent and trustworthy. Regulations which are followed over the whole life cycle of AI models, algorithms or the products they using these are therefore necessary as well as understandability or explainability of the decisions these models and algorithms made. Initiatives on every level of stakeholders started, e.g. international level on the European Union, country level, USA, China etc. as well on a company level. The post-hoc local interpretability methods could and should be implemented by economic decision makers to provide compliance with the regulations.",16,10.12775/EiP.2020.028,"This paper discusses the need for transparency and trust in AI models, particularly 'black box' systems, to enhance their acceptance and effectiveness. It reviews international initiatives and regulations aimed at making AI more interpretable and trustworthy, while proposing empirical tools for interpretability. The focus on regulations and interpretability in the context of AI's economic implications aligns it with Class A. Class: A",A,1612,75,stop,fp_0ba0d124f1
"With the explosive growth of data and the rapid rise of artificial intelligence and automated working processes, humans inevitably fall into increasingly close collaboration with machines as either employees or consumers. Problems in human-machine interaction arise as a consequence, not to mention the dilemmas posed by the need to manage information on ever-expanding scales. Considering the general superiority of machines in this latter respect, compared with human performance, it is essential to explore whether human-machine collaboration is valuable and, if so, why. Recent studies propose diverse explanation methods to uncover machine learning algorithms' ""black boxes,"" aiming to reduce human resistance and enhance efficiency. However, the findings of this literature stream have been inconclusive. Little is known about the influential factors involved or the rationale behind their impacts on human decision processes. We aimed to tackle these issues in the present study by specifically examining the joint impact of information complexity and machine explanations. Specifically, we cooperated with a large Asian microloan company to conduct a two -stage field experiment. Drawing upon studies in dual -process theories of reasoning that propose different conditions necessary to arouse humans' active information processing and systematic thinking, we tailored the treatments to vary the level of information complexity, the presence of collaboration, and the availability of machine explanations. We observed that, with large volumes of information and with machine explanations alone, human evaluators could not add extra value to the final collaborative outcomes. However, when extensive information was coupled with machine explanations, human involvement significantly reduced the default rate compared with machine -only decisions. We disentangled the underlying mechanisms with three -step empirical analyses. We reveal that the coexistence of large-scale information and machine explanations can invoke humans' active rethinking, which, in turn, shrinks gender gaps and increases prediction accuracy. In particular, we demonstrate that humans can spontaneously associate newly emerging features with others that had been overlooked but had the potential to correct the machine's mistakes. This capacity not only underscores the necessity of human-machine collaboration, but also offers insights into system designs. Our experiments and empirical findings provide nontrivial implications that are both theoretical and practical.",16,10.1287/isre.2023.0305,"This paper investigates the dynamics of human-machine collaboration, particularly in the context of decision-making with machine learning algorithms. It conducts a two-stage field experiment to assess how information complexity and machine explanations affect human evaluators' contributions to outcomes. The findings highlight the importance of human involvement in enhancing prediction accuracy and reducing biases, suggesting implications for system design. Since it analyzes the impact of AI on decision-making and fairness in outcomes, it aligns with Class A. Class: A",A,1768,92,stop,fp_0ba0d124f1
"Limited access to large-scale data is a key obstacle to building machine learning (ML) applications in practice, partly due to a reluctance of information exchange among data owners out of privacy and data security concerns. To address this ""information silo"" problem, federated learning (FL) techniques have been proposed to enable decentralized model training via an orchestrating central server and have received increasing attention in several industries (including healthcare and finance). Despite its superior privacy protection property, adoption of FL is limited by a lack of systematic understanding of its underlying economics. In this paper, we take an analytical approach to answer two questions: (1) when do data owners prefer to form a FL partnership over building ML models by themselves and (2) how can different contractual mechanisms be used to promote repeated contributions to FL (the cooperative outcome that benefits all participants). We formulate an iterated prisoner's dilemma (IPD) model that accounts for unique FL characteristics, including the specification of the payoff matrix and the involvement of a central server to sanction noncooperation. We find that partnership formation requires participants to be not too forward-looking in temporal preferences, which is contrary to the conventional wisdom in IPD. Furthermore, to promote repeated contributions, it is insufficient to only rely on penalties imposed by the central server or by participants for noncooperation, but a combination of both is enough. Our work advances theoretical understanding of the economics of FL and provides prescriptive insights that can inform FL participant selection and contract design.",16,10.1287/mnsc.2023.00611,"This paper analyzes the economics of federated learning (FL) to understand when data owners prefer FL partnerships over independent model building. It employs an iterated prisoner's dilemma model to explore contractual mechanisms that encourage cooperation among participants. The focus on economic implications and partnership dynamics aligns it with Class B, as it proposes methods for improving FL adoption rather than analyzing AI's impacts on fairness or ethical outcomes. Class: B",B,1647,81,stop,fp_0ba0d124f1
